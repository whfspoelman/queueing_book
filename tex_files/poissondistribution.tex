\section{Poisson Distribution}
\label{sec:poisson-distribution}

\subsection*{Theory and Exercises}


\Opensolutionfile{hint}
\Opensolutionfile{ans}

In this section we provide motivation for the use of the Poisson process as an arrival process of customers or jobs at a shop, service station, or machine, to receive service.  In the exercises we derive numerous  properties of  this exceedingly important distribution; in the rest of the book we will use these results time and again.

Consider a stream of customers that enter a shop over time. Let us write $N(s, t]$ for
the number of customers that entered during a time interval of $(s,t]$, and $N(t)=N(0,t]$. Clearly, as we do not know in advance how many customers will enter, we model $N(s,t]$ as a random variable for all
times $s$ and $t$.

Our first assumption is that the rate at which customers enter does not
change significantly over time. Then it is reasonable to assume that
the expected number of arrivals is proportional to the  length of
the interval. Hence, it is reasonable to assume that there exists some
constant~$\lambda$ such that
\begin{equation}
  \label{eq:6}
 \E{N(s,t]} = \lambda (t-s).
\end{equation}
The constant $\lambda$ is often called the \recall{arrival rate}.


The second assumption is that the process $\{N(s,t], s\leq t\}$ has
\recall{stationary} and \emph{independent increments}. Stationarity
means that the distributions of the number of arrivals are the same for
all intervals of equal length. Formally, $N(s_1,t_1]$ has the same
distribution as $N(s_2, t_2]$ if $t_2-s_2 = t_1-s_1$. Independence
means, roughly speaking, that knowing that $N(s_1,t_1]= n$, does not
help to make any predictions about $N(s_2, t_2]$ if the intervals
$(s_1,t_1]$ and $(s_2, t_2]$ have no overlap.

To find the distribution of $N(t)$, let us split the interval
$[0,t]$ into $n$ sub-intervals, all of equal length, and ask: `What is
the probability that a customer arrives in some given
sub-interval?'  By our second assumption, the arrival rate is
constant over time. Therefore, the probability $p$ of an arrival in each
interval should be constant. Moreover,
if the time intervals are very small, we can safely neglect the
probability that two or more customers arrive in one such tiny interval.

As a consequence, then, we can model the occurrence of an arrival in
some period $i$ as a Bernoulli distributed random variable $B_i$ such
that $p=\P{B_i =1}$ and $\P{B_i=0} = 1-\P{B_i = 1}$, and we assume that
$\{B_i\}$ are independent. The total number of arrivals $N_n(t)$ that
occur in $n$ intervals is then binomially distributed
\begin{equation}\label{eq:bin}
  \P{N_n(t) = k} = {n \choose k} p^k (1-p)^{n-k}.
\end{equation}

\begin{exercise}
Show that $\E{N_n(t)} = \sum_{i=1}^n \E{B_i} = n p$.
\begin{hint}
Use that $\E{X+Y} = \E X + \E Y$. 
\end{hint}
\begin{solution}
  \begin{equation*}
    \E{N_n(t)} = \E{\sum_{i=1}^n {B_i}} = \sum_{i=1}^n \E{B_i} = n \E{B_i} = n p.
  \end{equation*}
\end{solution}
\end{exercise}

\begin{exercise}
What is the difference between $N_n(t)$ and $N(t)$?
\begin{solution}
  $N_n(t)$ is a binomially distributed random variable with parameters
  $n$ and $p$. The maximum value of $N_n(t)$ is $n$. The random
  variable $N(t)$ models the number of arrivals that can occur during
  $[0,t]$. As such it is not necessarily bounded by $n$. Thus, $N_n(t)$ and $N(t)$ cannot represent the same random variable. 
\end{solution}
\end{exercise}


In fact, in some appropriate sense, $N_n(t)$ converges to $N(t)$ for $n\to \infty$ such
that
\begin{equation}\label{eq:pois}
  \P{N(t) = k} = 
e^{-\lambda t} \frac{(\lambda t)^k}{k!}.
\end{equation}
We say that $N(t)$ is \recall{Poisson distributed} with rate
$\lambda$, and write $N(t)\sim P(\lambda t)$. 

\begin{exercise}
  Show how the binomial distribution~\eqref{eq:bin} converges to the
  Poisson distribution~\eqref{eq:pois} if $n\to\infty$, $p\to0$ such
  that $n p=\lambda t$. 
  \begin{hint}
First find $p$, $n$, $\lambda$ and $t$ such that the rate
    at which an event occurs in both processes are the same. Then consider
    the binomial distribution and use the standard limit
    $(1-x/n)^n \to e^{-x}$ as $n\to \infty$. 
  \end{hint}
  \begin{solution} Now we like to relate $N_n(t)$ and $N(t)$. It is
    clear that we at least want the expectations to be the same, that
    is, $n p = \lambda t$. This implies that
\begin{equation*}
  p = \frac{\lambda t}n.
\end{equation*}
Substituting this in the expression for $\P{N_n(t)=k}$ gives
\begin{equation*}
  \P{N_n(t) = k} = {n \choose k} \left(\frac{\lambda t}{n}\right)^k \left(1-\frac{\lambda t}n\right)^{n-k}.
\end{equation*}
To see that 
\begin{equation*}\label{eq:52}
  \lim_{n\to\infty} {n \choose k} \left(\frac{\lambda t}{n}\right)^k \left(1-\frac{\lambda t}n\right)^{n-k} = e^{-\lambda t} \frac{(\lambda t)^k}{k!},
\end{equation*}
use that
    \begin{align*}
      {n \choose k} \left(\frac{\lambda t}{n}\right)^k \left(1-\frac{\lambda t}n\right)^{n-k} 
&= \frac{n!}{k!(n-k)!} \left(\frac{\lambda t}{n}\frac{n}{n-\lambda t}\right)^k \left(1-\frac{\lambda t}n\right)^{n} \\
&= \frac{(\lambda t)^k}{k!} \left(\frac n{n-\lambda t} \right)^k  \frac{n!}{n^k(n-k)!}\left(1-\frac{\lambda t}n\right)^{n}\\
&= \frac{(\lambda t)^k}{k!} \left(\frac n{n-\lambda t} \right)^k \frac{n}{n}\frac{n-1}{n}\cdots\frac{n-k+1}{n} \left(1-\frac{\lambda t}n\right)^{n}.
\end{align*}
Observe now that, as $\lambda t$ is finite, $n/(n-\lambda t)\to 1$ as
$n\to \infty$. Also for any finite $k$, $(n-k)/n\to1$. Finally, we use
that $(1+x/n)^n\to e^{x}$ so that ,
$\left(1-\frac{\lambda t}n\right)^{n} \to e^{-\lambda t}$.  The rest
is easy, so that, as $n\to\infty$,  the above converges to 
\begin{equation*}
\frac{(\lambda t)^k}{k!} e^{-\lambda t}.
\end{equation*}

  \end{solution}
\end{exercise}

Thus, if we assume that the arrival rate of customers is constant for some amount of time, e.g., an hour, and we assume that the probability of a customer arriving in some very small amount of time,  e.g., a millisecond, is small, the resulting number of arrivals of customers is Poisson distributed. 


\begin{exercise}\label{ex:2}
  Show that if $N(t)\sim P(\lambda t)$, the expected number of arrivals during $[0,t]$ is $\E{N(t)} = \lambda t$.
  \begin{hint}
Apply the definition of the expectation, and in the computations use that 
\begin{equation*}
\sum_{n=0}^\infty n \frac{\lambda^n}{n!} = \sum_{n=1}^\infty n \frac{\lambda^n}{n!} = \sum_{n=1}^\infty \frac{\lambda^n}{(n-1)!} = \lambda \sum_{n=0}^\infty \frac{\lambda^n}{n!} = \lambda e^{\lambda}.
\end{equation*}
  \end{hint}
  \begin{solution} 
    When a random variable~$N$ is Poisson distributed with parameter
    $\lambda t$,
    \begin{align*}
      \E N 
&= \sum_{n=0}^\infty n e^{-\lambda t}\frac{(\lambda t)^n}{n!}  \\
&= \sum_{n=1}^\infty n e^{-\lambda t}\frac{(\lambda t)^n}{n!}, \text{ since the term with $n=0$ cannot contribute} \\
&= e^{-\lambda t} \lambda t \sum_{n=1}^\infty \frac{(\lambda t)^{n-1}}{(n-1)!} \\
&= e^{-\lambda t} \lambda t \sum_{n=0}^\infty \frac{(\lambda t)^{n}}{n!}, \text{ by a change of variable}\\
&= e^{-\lambda t} \lambda t e^{\lambda t} \\
&= \lambda t.
    \end{align*}
\end{solution}
\end{exercise}


\begin{exercise}
  Show that if $N(t)\sim P(\lambda t)$, the variance of the number of arrivals during $[0,t]$ is $\V{N(t)} = \lambda t$. 
  \begin{hint}
Realize that $\V N = \E{N^2} - (\E N)^2$. Compute $\E{N^2}$, use Exercise~\ref{ex:2} for $(\E{N})^2$ and the hint of Exercise~\ref{ex:2} to simplify the calculations. 
  \end{hint}
  \begin{solution} 

    \begin{align*}
      \E{N^2}
&= \sum_{n=0}^\infty n^2 e^{-\lambda t}\frac{(\lambda t)^n}{n!}  \\
&= e^{-\lambda t} \sum_{n=1}^\infty n \frac{(\lambda t)^n}{(n-1)!}  \\
&= e^{-\lambda t} \sum_{n=0}^\infty (n+1) \frac{(\lambda t)^{n+1}}{n!}  \\
&= e^{-\lambda t} \lambda t \sum_{n=0}^\infty n \frac{(\lambda t)^{n}}{n!}  +e^{-\lambda t}\lambda t \sum_{n=0}^\infty\frac{(\lambda t)^{n}}{n!}  \\
&\quad\text{now use the hint to simplify the first summation} \\
&= (\lambda t)^2  + \lambda t.
\end{align*}
Hence, $\V N = \E{N^2} - (\E N)^2 = (\lambda t)^2 + \lambda t - (\lambda t)^2 = \lambda t$.
\end{solution}
\end{exercise}

Recall that the \recall{moment generating function}  of a random variable $X\geq 0$ and $s$ a real number is defined as
\begin{equation*}
  M_X(s) = \E{e^{s X}}.
\end{equation*}
With the moment-generating function we compute the mean and variance of $N(t)$ with less effort. 


\begin{exercise}
Show that the moment-generating function of the random variable~$N(t)\sim P(\lambda t)$ is
\begin{equation*}
M_{N(t)}(s) 
%=  \E{e^{sN(t)}}  
= \exp{(\lambda t(e^s-1))}.
\end{equation*}
  \begin{hint}Observe first that for a random variable $X\in\N$ and some function $f$, we have that
    \begin{equation*}
\E{f(X)}=\sum_{n=0}^\infty f(n) \P{X=n}.      
    \end{equation*}
Then take the function $f(x)=e^{s x}$. Finally, use that $\P{N(t)=k}=e^{-\lambda t}(\lambda t)^k/k!$.
  \end{hint}
\begin{solution}
Since $N(t)$ is Poisson distributed with parameter $\lambda t$, 
\begin{align*}
M_{N(t)}(s)
&=  \E{e^{s N(t)}} \\
&= \sum_{k=0}^\infty e^{s k} \P{N(t)=k} \\
&\quad\text{where we use that $\E{f(N(t))}=\sum_{n=0}^\infty f(n) \P{N(t)=n}$}\\
&= \sum_{k=0}^\infty e^{s k} \frac{(\lambda t)^k}{k!} e^{-\lambda t}  \\
&= e^{-\lambda t} \sum_{k=0}^\infty  \frac{(e^s \lambda t)^k}{k!}  \\
&= \exp(-\lambda t + e^s \lambda t) =\exp(\lambda t(e^s - 1)).
\end{align*}
\end{solution}
\end{exercise}

\begin{exercise}
Use  the moment-generating function of $N(t)$ to compute $\E{N(t)}$ and $\V{N(t)}$. 
\begin{hint}
  Recall that $M_{N(t)}'(0) = \d M_{N(t)}(s)/\d s|_{s=0} = \E{N(t)}$ and $M_{N(t)}''(0)= \E{(N(t))^2}$. 
\end{hint}
\begin{solution}
Using the expression for the moment-generating function of the previous exercise,
  \begin{equation*}
    M_{N(t)}'(s) = \lambda t e^s \exp(\lambda t(e^s - 1)).
  \end{equation*}
Hence $\E{N(t)} = M_{N(t)}'(0) = \lambda t $. Next, 
  \begin{equation*}
    M_{N(t)}''(s) = (\lambda t e^s + (\lambda t e^s)^2) \exp(\lambda t(e^s - 1)),
  \end{equation*}
hence $\E{(N(t))^2} = M''(0) = \lambda t + (\lambda t)^2$. And thus, 
\begin{equation*}
\V{N(t)} =\E{(N(t))^2}-(\E{N(t)})^2 = \lambda t + (\lambda t)^2 - (\lambda t)^2 = \lambda t.
\end{equation*}
\end{solution}
\end{exercise}

Define the \emph{square coefficient  of variation (SCV)} of a random variable~$X$ as 
\begin{equation}\label{eq:62}
  C^2= \frac{\V X}{(\E X)^2}.
\end{equation}
As  will become clear later, the SCV is a very important concept in
  queueing theory. Memorize it as a measure of \emph{relative
  variability}.

\begin{exercise}
Show that   the SCV of $N(t)$ is equal to $1/\lambda t$. 
  \begin{solution}
    \begin{equation*}
SCV = \frac{\V{N(t)}}{(\E{N(t)})^2} = \frac{\lambda t}{(\lambda t)^2} = \frac1{\lambda t}.
    \end{equation*}

  \end{solution}
\end{exercise}

For the next couple of exercises, we need the concept of conditional probability. To help you recall this, consider the next question.

\begin{exercise} \label{ex:18}
  We have  one gift to give to one of three children. As we cannot
  divide the gift into parts, we decide to let `fate decide'. That
  is, we choose a random number in the set $\{1, 2, 3\}$. The first
  child that guesses the number wins the gift. Show that the
  probability of winning the gift is the same for each child.
  \begin{hint}
    For the second child, condition on the event that the first does not choose the right number.
    Use the definition of conditional probability:
    $\P{A|B} = \P{AB}/\P{B}$ provided $\P{B}>0$.
  \end{hint}
\begin{solution}
    The probability that the first child to guess also wins is
    $1/3$. What is the probability for child number two? Well, for
    him/her to win, it is necessary that child one does not win and
    that child two guesses the right number of the remaining
    numbers. Assume, without loss of generality that child 1 chooses
    $3$ and that this is not the right number. Then 
    \begin{equation*}
      \begin{split}
&\P{\text{Child  2 wins}} \\
&= \P{\text{Child 2 guesses the right number and child 1 does not win}} \\
&= \P{\text{Child 2 guesses the right number} \given \text{ child 1 does not win}}
\cdot \P{\text{Child 1 does not win}} \\
&= \P{\text{Child 2 makes the right guess in the set $\{1,2\}$}}\cdot \frac 23 \\
&= \frac 1 2\cdot \frac 23  = \frac 1 3.
      \end{split}
    \end{equation*}
    Similar conditional reasoning gives that child 3 wins with probability $1/3$. 
  \end{solution}
\end{exercise}

Below, and elsewhere, we use the following efficient notation:  the function $f=o(h)$ means that $f$ is such $f(h)/h \to 0$ as $h\to 0$.  The next exercise helps you practice with this notation.
\begin{exercise}
  Let $c$ be a constant (in $\R$) and the functions $f$ and $g$ both of $o(h)$. Then show that (1) $f(h) \to 0$ when $h\to 0$, (2) $c\cdot f = o(h)$, (3) $f+g=o(h)$, and (4) $f\cdot g=o(h)$. 
 \begin{solution}
In fact (1) is trivial: $|f(h)| \leq |f(h)/h|$ when $|h| < 1$. But it is given that the right hand side goes to zero.  For (2) and (3):
\begin{align*}
\lim_{h\to 0} \frac{c f(h)}{h} &=  c \lim_{h\to 0} \frac{f(h)}{h} = 0, \; \text{as } f = o(h), \\
\lim_{h\to 0} \frac{f(h) + g(h)} h &= \lim_{h\to 0} \frac{f(h)} h + \lim_{h\to 0} \frac{g(h)} h = 0.
\end{align*}
For (4), use  the Algebraic Limit Theorem,
\begin{align*}
\lim_{h\to 0} \frac{f(h)g(h)}{h} &= \lim_{h\to 0} h \frac{f(h)}{h} \frac{g(h)}{h} \\
&= \lim_{h\to 0} h \lim_{h\to 0} \frac{f(h)}{h} \lim_{h\to 0} \frac{g(h)}{h} \\
&= 0 \cdot 0 \cdot 0 = 0.
\end{align*}
  \end{solution}
\end{exercise}


\begin{exercise}
  Show that if $N(t) \sim P(\lambda t)$, we have for $0<h\ll 1$,
  \begin{equation*}
\P{N(t+h) = n \given N(t) = n} = 1-\lambda h + o(h).
  \end{equation*}
    \begin{hint}
Use  the definition of the conditional probability of Exercise~\ref{ex:18}. 

Think about the meaning of the formula $\P{N(t+h) = n \given N(t) = n}$.
It is a conditional probability that should be read like this: given that up to time $t$ we have seen $n$ arrivals (i.e., $N(t)=n$), what is the probability that just a little later (at $t+h$) the number of arrivals is still $n$, i.e., $N(t+h)=n$?
Then use the definition of the Poisson distribution to compute this probability.
Finally, use Taylor's expansion of $e^{x}$ to see that $e^{x} = 1 +x + o(x)$ for $|x| \ll 1$.
    \end{hint}
\begin{solution}
Write $N(s, t]$ for the number of arrivals in the interval $(s,t]$. First we make a few simple observations: $N(t+h]= N(t) + N(t, t+h]$, hence
\begin{equation*}
  \begin{split}
  \1{N(t+h)=n, N(t)=n}
&=  \1{N(t) + N(t, t+h] = n, N(t)=n} = \\
&\1{N(t, t+h] = 0, N(t)=n}.
  \end{split}
\end{equation*}
Thus, 
    \begin{align*}
  \P{N(t+h) = n | N(t) = n} 
&=  \P{N(t+h) = n, N(t) = n}/\P{N(t)=n} \\
&=  \P{N(t, t+h] = 0,  N(t) = n}/\P{N(t)=n} \\
&=  \P{N(t, t+h] = 0} \P{N(t) = n}/\P{N(t)=n}, \\
& \quad\text{(by independence of } N(t, t+h] \text{ and } N(t))\\
&= \P{N(t, t+h] = 0} \\
&= \P{N(0, h] = 0} \\
&\quad\text{(by stationarity of the Poisson process)}\\
&= e^{-\lambda h} (\lambda h)^0/0! \\
&= e^{-\lambda h} = 1-\lambda h + o(h).
    \end{align*}
\end{solution}
\end{exercise}

\begin{exercise}
  Show that if $N(t) \sim P(\lambda t)$, we have for small $h$,
 $\P{N(t+h) = n+1 \given N(t) = n} = \lambda h + o(h)$. 
    \begin{hint} Use the previous exercise.
    \end{hint}
\begin{solution}
  \begin{align*}
  \P{N(t+h) = n +1 | N(t) = n} 
&=  \P{N(t+h) = n +1 , N(t) =n}/\P{N(t) = n}\\
&= \P{N(t, t+h] = 1} = e^{-\lambda h} (\lambda h)^1/1! \\
&= (1-\lambda h + o(h))\lambda h  = \lambda h - \lambda^2 h^2 + o(h) \\
&= \lambda h + o(h). 
  \end{align*}
\end{solution}
\end{exercise}

\begin{exercise}
  Show that if $N(t) \sim P(\lambda t)$, we have for small $h$,
 $\P{N(t+h) \geq n+2 \given N(t) = n} = o(h)$.
    \begin{hint}
 Use that  $\sum_{i=2}^\infty x^i/i! = \sum_{i=0}^\infty x^i/i! - x -1 = e^x -x - 1$.
    \end{hint}
\begin{solution}
  \begin{align*}
  \P{N(t+h) \geq n+2 | N(t) = n} 
&= \P{N(t, t+h] \geq 2} \\
&= e^{-\lambda h} \sum_{i=2}^\infty \frac{(\lambda h)^i}{i!} 
= e^{-\lambda h} \left(\sum_{i=0}^\infty \frac{(\lambda h)^i}{i!} - \lambda h - 1\right)\\
&= e^{-\lambda h}(e^{\lambda h} - 1 - \lambda h) 
= 1 - e^{-\lambda h}(1 + \lambda h) \\
&= 1 - (1-\lambda h + o(h))(1+\lambda h) 
= 1 - (1-\lambda^2 h^2 + o(h)) \\
&= \lambda^2 h^2 + o(h) = o(h).
  \end{align*}

We can also use the results of the previous parts to see that
\begin{align*}
  \P{N(t+h) \geq n+2 | N(t) = n} 
&= \P{N(t, t+h] \geq 2} = 1- \P{N(t, t+h]<2} \\
&= 1 - \P{N(t, t+h]= 0} - \P{N(t, t+h]=1} \\
&= 1 - (1-\lambda h) + o(h) ) - (\lambda h + o(h)) \\
&= o(h).
\end{align*}
\end{solution}
\end{exercise}

We define the Poisson \emph{process} as the set $N=\{N(t), t\geq 0\}$. Observe that this process is a much more complicated object than the Poisson distributed random variable $N(t)$. The
process is an uncountable set of random variables indexed by  $t\in \R^+$, whereas $N(t)$ is just \emph{one} random variable.

\begin{exercise}
Show that  for a Poisson process $N$, 
\begin{equation*}
\P{N(s) =1\given N(t)=1} = \frac s t,
\end{equation*}
if $s\in[0,t]$. Thus, if you know that an arrival occurred during $[0,t]$, the arrival is distributed
uniformly on the interval $[0,t]$.
\begin{hint}
 Observe that 
  \begin{equation*}
\{N(0,s]+N(s,t]=1\}\cap\{N(0,s]=1\} = \{1+N(s,t]=1\}\cap\{N(0,s]=1\}=\{N(s,t]=0\}\cap\{N(0,s]=1\}.
  \end{equation*}
Next use that these two events are independent.
\end{hint}
\begin{solution}
From the hint,
\begin{align*}
  \P{N(0,s] =1\given N(0,t]=1} 
&= \frac{\P{N(0,s] =1, N(0,t]=1}}{\P{N(0,t]=1}} \\
&= \frac{\P{N(0,s] =1, N(s,t]=0}}{\P{N(0,t]=1}} \\
&= \frac{\P{N(0,s] =1}\P{N(s,t]=0}}{\P{N(0,t]=1}} \\
&= \frac{\lambda s e^{-\lambda s} e^{-\lambda (t-s)}}{\lambda t e^{-\lambda t}} = \frac s t.
\end{align*}
\end{solution}
\end{exercise}

 


\recall{Merging}, or the \emph{superposition}, of Poisson processes
occurs often in practice.  We have two Poisson processes, for
instance, the arrival processes $N_\lambda$ of men and $N_\mu$ women
at a shop. In the figure below, each cross represents an arrival, in
the upper line it corresponds to a man, in the middle line to a woman and in
the lower line to an arrival of a general customer at the shop. Thus, the
shop `sees' the superposition of these two arrival
processes.




  \begin{center}
\begin{tikzpicture}[scale=1]
%\draw[[-{Triangle[open]},dotted] (0,10)--(8.5,10);

\draw[->] (0,2)--(10,2);
\node[left] at (0,2) {$N_\lambda(t)$};
\draw[->] (0,1)--(10,1);
\node[left] at (0,1) {$N_\mu(t)$};
\draw[->] (0,0)--(10,0);
\node[left] at (0,0) {$N_{\lambda+\mu}(t)$};

\draw[{Rays[]}-{Rays[]},dotted] (1,2.06)--(1,-0.06);
\draw[{Rays[]}-{Rays[]},dotted] (1.5,1.06)--(1.5,-0.06);
\draw[{Rays[]}-{Rays[]},dotted] (3.2,2.06)--(3.2,-0.06);
\draw[{Rays[]}-{Rays[]},dotted] (3.5,1.06)--(3.5,-0.06);
\draw[{Rays[]}-{Rays[]},dotted] (4.5,1.06)--(4.5,-0.06);
\draw[{Rays[]}-{Rays[]},dotted] (5,1.06)--(5,-0.06);
\draw[{Rays[]}-{Rays[]},dotted] (6.1,1.06)--(6.1,-0.06);
\draw[{Rays[]}-{Rays[]},dotted] (7.1,2.06)--(7.1,-0.06);
\end{tikzpicture}
\end{center}

For the next set of exercises,  assume that $N_\lambda(t)\sim \text{P}(\lambda t)$, $N_\mu(t) \sim \text{P}(\mu t)$ and are independent.

\begin{exercise} 
Show that  $N_\lambda(t) + N_\mu(t) \sim \text{P}((\lambda + \mu)t)$ for all $t\geq 0$. Thus, when we merge two independent Poisson processes, the merged process $\{N_{\lambda+\mu}(t)\}$ is also a Poisson process whose rate is the sum of the rates of the original Poisson processes.
  \begin{hint}
Use a conditioning argument or use moment generating functions.

In particular, for
    conditioning, use that $\P{AB} = \P{A\given B}\P{B}$. More
    generally, if the set $A$ can be split into disjoint sets $B_i$,
    i.e., $A=\bigcup_{i=1}^n B_i$, then
    \begin{equation*}
      \P{A} = \sum_{i=1}^n \P{A B_i} = \sum_{i=1}^n \P{A\given B_i} \P{B_i},
    \end{equation*}
    where we use the conditioning formula to see that
    $\P{A B_i} = \P{A \given B_i} \P{B_i}$.  Now choose practical sets
    $B_i$.  

In the computations, use the standard binomial formula $(a+b)^n = \sum_{i=0}^n {n \choose i} a^{n-i} b^i$. 
  \end{hint}
    \begin{solution}
Choose the \emph{sets} $B_i$ as $\{N_\lambda(t) = i\}$. Then, the set $\{N_\lambda(t) + N_\mu(t) = n\}$ can be decomposed into the subsets $B_i$, i.e., 
\begin{equation*}
\{N_\lambda(t) + N_\mu(t) = n\} =  \cup_{i=0}^n \{N_\lambda(t) = i\}.
\end{equation*}
To see this, consider an example. Cats arrive at rate $\lambda$, dogs at rate $\mu$. Suppose that $5$ animals (cats plus dogs) arrived. Then  there are 6 possibilities to form a set with 5 animals in total: 0 cats + 5 dogs, 1 cat + 4 dogs, \ldots, 5 cats + 0 dogs. The right-hand side of the equality decomposes  the fact that $5$ animals arrive into sets in which we know that the number of cats is $0, 1, 2,\ldots, 5$. Also,  if we know the number of cats, we also know the number of dogs, as this is $5$ minus the number of cats. %Hence, the same set is created and the equality holds. 

Now that we know how to conveniently split up the set $\{N_\lambda(t) + N_\mu(t) = n\}$,  we can use conditioning: 
  \begin{equation*}
    \begin{split}
\P{N_\lambda(t) + N_\mu(t) = n} 
&= \sum_{i=0}^n \P{N_\lambda(t) + N_\mu(t) = n|N_\lambda(t) = i}\P{N_\lambda(t)=i}.
\end{split}
\end{equation*}
Now, if $N_\lambda(t)=i$, then 
\begin{equation*}
N_\lambda(t)+N_\mu(t) = n \iff 
i+N_\mu(t) = n \iff 
N_\mu(t) = n-i.
\end{equation*}
With this, 
  \begin{align*}\label{eq:002}
\P{N_\lambda(t) + N_\mu(t) = n} 
&= \sum_{i=0}^n \P{ N_\mu(t) = n-i}\P{N_\lambda(t)=i} \\
&= \sum_{i=0}^n \frac{(\mu t)^{n-i}}{(n-i)!} \frac{(\lambda t)^i}{i!} e^{-(\mu+\lambda)t} \\
&= e^{-(\mu+\lambda)t} \sum_{i=0}^n \frac{(\mu t)^{n-i}}{(n-i)!} \frac{(\lambda t)^i}{i!}  \\
&= e^{-(\mu+\lambda)t}\frac 1{n!} \sum_{i=0}^n {n \choose i }(\mu t)^{n-i}(\lambda t)^i, \quad\text{ (binomial formula)}   \\
&= \frac{((\mu+\lambda)t)^n}{n!}e^{-(\mu+\lambda)t}.
  \end{align*}

  Now with the moment generating functions of $N_\lambda(t)$ and $N_\mu(t)$, and using that by the independence of $N_\lambda(t)$ and $N_\mu(t)$, the moment generating function of $N_\lambda + N_\mu$ is the product of the moment generating functions of $N_\lambda$ and $N_\mu$,
\begin{align*}
M_{N_\lambda(t)+N_\mu(t)}(s) 
&= M_{N_\lambda(t)}(s) M_{N_{\mu}(t)}(s) \\
&=\exp(\lambda t (e^s -1)) \exp(\mu t(e^s-1)) \\
&= \exp((\lambda + \mu)t (e^s-1)).
\end{align*}
Finally, since  the moment generating function uniquely characterizes
the distribution, and since the above expression has the same form as a Poisson random variable  with parameter $(\lambda+\mu)t$, we can conclude that $N(t)\sim P((\lambda +\mu) t)$.
    \end{solution}
\end{exercise}

\begin{comment}
  
\begin{exercise}
What is the
  meaning of the event
  $\{N_\lambda(h) = 1, N_\mu(h) = 0\}\cap\{N_\lambda(h) + N_\mu(h) = 1\}$?
    \begin{solution}
There are two arrival processes, arrivals of the first type occur with rate $\lambda$, arrivals of the second  type with rate $\mu$.  If the event  $\{N_\lambda(h) + N_\mu(h) = 1\}$ is true, it tells us that precisely one arrival occurred.  The event $\{N_\lambda(h) = 1, N_\mu(h) = 0\}$ means that up to time $t$, only an arrival of the first type occurred. Thus, the intersection of these events means that an arrival occurred \emph{and} that this arrival was of the first type. 
    \end{solution}
\end{exercise}
\end{comment}


\begin{exercise} 
Show that, given that an arrival occurred,  the probability that the arrival was of the first type is  
 \begin{equation*}
    \P{N_\lambda(h) = 1 | N_\lambda(h) + N_\mu(h) = 1} =
\frac{\lambda}{\lambda+\mu}.
    \end{equation*}
    Note that the right-hand-side does not depend on $h$, hence it
    holds for any time $h$, whether it is small or not.  
    \begin{hint}
Use the standard formula for conditional probability and that  $N_\lambda(t) + N_\mu(t) \sim \text{P}((\lambda + \mu)t)$.
% Suppose
%       we write $N(t)=N_\lambda(t) + N_\mu(t)$. Then
%       \begin{equation*}
%       \P{N_\lambda(h) = 1| N(h) = 1}
%       \end{equation*}
%       is the probability that $N_\lambda(h)=1$ and $N_\mu(h)=0$ \emph{given}
%       that $N(t)=1$. Use the standard formula for conditional probability. 
    \end{hint}
    \begin{solution}
  With the above:
  \begin{align*}
&    \P{N_\lambda(h) = 1 | N_\lambda(h) + N_\mu(h) = 1} \\
&= \frac{\P{N_\lambda(h) = 1,  N_\lambda(h) + N_\mu(h) = 1} }{ \P{N_\lambda(h) + N_\mu(h) = 1}} \\ 
&= \frac{\P{N_\lambda(h) = 1,  N_\mu(h) = 0}}{ \P{N_{\lambda+\mu}(h) = 1}} \\ 
&= \frac{\P{N_\lambda(h) = 1}\P{N_\mu(h) = 0}}{ \P{N_{\lambda+\mu}(h)  = 1}} \\ 
&= \frac{\lambda h \exp(-\lambda h) \exp(-\mu h)}{((\lambda+\mu)h)\exp{(-(\lambda+\mu)h)}}\\
&= \frac{\lambda h \exp{(-(\lambda + \mu)h)}}{((\lambda+\mu)h)\exp{(-(\lambda+\mu)h)}}\\
&= \frac{\lambda}{\lambda+\mu}.
  \end{align*}
    \end{solution}
\end{exercise}


Besides merging Poisson streams, we can also consider the concept of \emph{splitting}, or \emph{thinning}, a stream into sub-streams, as follows.  When a customer arrives, throw a coin that lands   heads with probability $p$ and tails with $q=1-p$. When the coin   lands heads, the customer is of type 1 (e.g., a woman), otherwise of type 2 (e.g., a child).  Another   way of thinning is by modeling the stream of people passing a shop   as a Poisson process with rate $\lambda$. With probability $p$ a   person decides, independent of anything else, to enter the shop,  c.f. the figure below. The crosses at the upper line are passersby  in the street. The crosses at the lower line are the customers that   enter the shop. The outcome of the $k$th throw of a coin is   indicated by $B_k$. In the figure below,  $B_1=1$ so that the first passerby turns into a
  customer entering the shop; the second passerby does not enter as   $B_2=0$, and so on. Like this, the stream of passersby is thinned   with Bernoulli distributed random variables.

  \begin{center}
\begin{tikzpicture}[scale=1]
%\draw[[-{Triangle[open]},dotted] (0,10)--(8.5,10);
\draw[->] (0,2)--(10,2);
\node[left] at (0,2) {$N_\lambda(t)$};
%\draw[->] (0,1)--(10,1);
%\node[left] at (0,1) {$B_k$};
\draw[->] (0,0)--(10,0);
\node[left] at (0,0) {$N_{\lambda p}(t)$};

\draw[{Rays[]}-{Rays[]},dotted] (1,2.06)--(1,-0.06) 
node[below]  {$B_1=1$};

\draw[{Rays[]}-{Circle[open]},dotted] (2.5,2.06)--(2.5,1.3) 
node[below] {$B_2=0$};

\draw[{Rays[]}-{Circle[open]},dotted] (4,2.06)--(4,1.3) 
node[below, fill=white] {$B_3=0$};

\draw[{Rays[]}-{Rays[]},dotted] (5,2.06)--(5,-0.06) 
node[below]  {$B_4=1$};

\draw[{Rays[]}-{Rays[]},dotted] (6.5,2.06)--(6.5,-0.06) 
node[below]  {$B_5=1$};


\draw[{Rays[]}-{Circle[open]},dotted] (7.5,2.06)--(7.5,1.3) 
node[below, fill=white] {$B_6=0$};

\end{tikzpicture}
  \end{center}


\begin{exercise}\label{ex:1}
Show that the Poisson process obtained by thinning the original
  process is $\sim P(\lambda t p)$ for any~$t$.
  \begin{hint}
There are (at least) two ways to get this result. 

Method 1:  Condition on the total number of arrivals $N(t)=n$ up to time
      $t$. Then, realize that the probability that a person is of type 1 is $p$. Hence when you consider $n$ people in
      total, the number $N_1(t)$ of type 1 people is binomially distributed. Thus, given that $n$ people arrived, the probability of $k$ `successes' (i.e., arrivals of type 1), is 
      \begin{equation*}
        \P{N_1=k \given N = n} = {n \choose k} p^k (1-p)^{n-k}.
      \end{equation*}

      Again use that if the set $A$ can be split into disjoint sets
      $B_i$, i.e., $A=\bigcup_{i=1}^n B_i$, then
    \begin{equation*}
      \P{A} = \sum_{i=1}^n \P{A\given B_i} \P{B_i}.
    \end{equation*}
Now choose practical sets $B_i$ and use $\sum_{k=0}^{\infty} \frac{x^k}{k!} = e^x$. 

Method 2: Dropping the dependence of $N$ on $t$ for the moment for notational convenience, you might also consider the random variable 
  \begin{equation*}
    Y = \sum_{i=1}^N Z_i,
  \end{equation*}
  with $N\sim P(\lambda)$ and $Z_i\sim B(p)$. Show that the moment
  generating function of $Y$ is equal to the moment generating
  function of a Poisson random variable with parameter $\lambda p$.
  \end{hint}
    \begin{solution}
Method 1: Suppose that  $N_1$ is the  thinned stream, and $N$ the total stream. Then, 
\begin{align*}
    \P{N_1 = k}
&= \sum_{n=k}^\infty \P{N_1 =k, N = n} 
= \sum_{n=k}^\infty \P{N_1 =k\given N = n}\P{N=n} \\
&= \sum_{n=k}^\infty \P{N_1 =k\given N = n}e^{-\lambda} \frac{\lambda^n}{n!}\\
&= \sum_{n=k}^\infty {n \choose k} p^k (1-p)^{n-k} e^{-\lambda} \frac{\lambda^n}{n!}, \quad\text{by the hint}\\
&= e^{-\lambda}\sum_{n=k}^\infty  \frac{p^k (1-p)^{n-k}}{k! (n-k)!} \lambda^n
= e^{-\lambda} \frac{(\lambda p)^k}{k!} \sum_{n=k}^\infty  \frac{(\lambda (1-p))^{n-k}}{(n-k)!}\\
&= e^{-\lambda} \frac{(\lambda p)^k}{k!} \sum_{n=0}^\infty  \frac{(\lambda (1-p))^{n}}{n!}
= e^{-\lambda} \frac{(\lambda p)^k}{k!} e^{\lambda(1-p)} \\
&= e^{-\lambda p} \frac{(\lambda p)^k}{k!}.
\end{align*}
We see that the thinned stream is Poisson with parameter $\lambda p$. If we introduce the $t$ again, we obtain $P(\lambda t p)$.

Method 2: Now consider $Y=\sum_{i=1}^N Z_i$. Suppose that $N=n$, so that $n$
arrivals occurred. Then we throw $n$ independent coins with success probability
$p$. It is clear that $Y$ is indeed a thinned Poisson random variable.

Model the coins as a generic Bernoulli distributed random variable
$Z$.  We first need
\begin{equation*}
  \E{e^{s Z}} = e^0 \P{Z=0} + e^{s} \P{Z=1} = (1-p) + e^s p.
\end{equation*}
Suppose that $N=n$, then since the outcomes $Z_i$ of the coins are i.i.d.,
\begin{equation*}
\E{e^{s\sum_{i=1}^n Z_i}} = \left(\E{e^{s Z}}\right)^n = \left(1 + p (e^s - 1)\right)^n,
\end{equation*}
where we again use that the moment generating function of the sum of independent random variables is the product of the moment generating functions.

Now for a random variable $X\in N$ observe that $X=\sum_{n=0}^\infty X\1{X=n} = \sum_{n=0}^\infty n \1{X=n}$. To see this, note first that $X\1{X=n} = n\1{X=n}$ because $X=n$ when $\1{X=n} =1$, and second that $\sum_{n=0}^\infty \1{X=n} =1$, since $X$ takes one of the values in $\N$, and events $\{X=n\}$ and $\{X=m\}$ are non-overlapping when $n\neq m$. 

With this idea, 
\begin{align*}
  \E{e^{s Y}}
&= \E{\sum_{n=0}^\infty e^{s\sum_{i=1}^N Z_i} \1{N=n}} \\
&= \E{\sum_{n=0}^\infty e^{s\sum_{i=1}^n Z_i} \1{N=n}} \\
&= \sum_{n=0}^\infty \E{e^{s\sum_{i=1}^n Z_i} \1{N=n}} \\
&= \sum_{n=0}^\infty \E{e^{s\sum_{i=1}^n Z_i}} \E{\1{N=n}}, \text{ by independence of $Z_i$ and $N$}, \\
&= \sum_{n=0}^\infty \left(1+p(e^s-1)\right)^n \P{N=n} \\
&= \sum_{n=0}^\infty \left(1+p(e^s-1)\right)^n e^{-\lambda} \frac{\lambda^n}{n!}
= e^{-\lambda} \sum_{n=0}^\infty \frac{\left(1+p(e^s-1)\right)^n \lambda^n}{n!}\\
&= e^{-\lambda} \exp(\lambda (1+p(e^s-1))) = \exp(\lambda p (e^s - 1)).
\end{align*}
Thus, $Y$ has the same moment-generating function as a Poisson
distributed random variable with parameter $\lambda p$. Since
the moment-generating function specifies the distribution uniquely,
$Y\sim P(\lambda p)$.
\end{solution}
\end{exercise}    

The concepts of merging and thinning are  useful to analyze queueing
  networks. Suppose the departure stream of a machine splits into
  two sub-streams, e.g., a fraction $p$ of the jobs moves on to another
  machine and the rest ($1-p$) of the jobs leaves the system. Then we
  can model the arrival stream at the second machine as a thinned
  stream (with probability $p$) of the departures of the first
  machine. Merging occurs where the output streams of various stations arrive at another station. 


\Closesolutionfile{hint}
\Closesolutionfile{ans}

\opt{solutionfiles}{
\subsection*{Hints}
\input{hint}
\subsection*{Solutions}
\input{ans}
}

%\clearpage

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../queueing_book"
%%% End:
